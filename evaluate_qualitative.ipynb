{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation of Model Performance\n",
    "\n",
    "Given the same prefix sequence of notes, does adding expressive features change model output? That is, do models respect expressive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Some constants like filepaths and encodings for running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data directory\n",
    "DATA_DIR = \"/home/pnlong/musescore/datav\"\n",
    "SHOW_SEQUENCES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from os.path import exists, basename\n",
    "from os import makedirs, remove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import representation\n",
    "from IPython.display import display, HTML, Audio\n",
    "from time import perf_counter\n",
    "import utils\n",
    "import torch\n",
    "import dataset\n",
    "import music_x_transformers\n",
    "import train\n",
    "import encode\n",
    "import decode\n",
    "from read_mscz.read_mscz import read_musescore\n",
    "from read_mscz.music import BetterMusic\n",
    "\n",
    "# filepaths\n",
    "TEST_DATA_DIR = \"/home/pnlong/musescore/test_data/evalqual\"\n",
    "if not exists(TEST_DATA_DIR):\n",
    "    makedirs(TEST_DATA_DIR)\n",
    "PREFIX_MSCZ_FILEPATH = f\"{TEST_DATA_DIR}/simple.mscz\"\n",
    "if not exists(PREFIX_MSCZ_FILEPATH):\n",
    "    raise FileNotFoundError(\"Must provide a valid MuseScore prefix filepath.\")\n",
    "PREFIX_OUTPUT = basename(PREFIX_MSCZ_FILEPATH).split(\".\")[0]\n",
    "\n",
    "# load the encoding\n",
    "encoding = representation.load_encoding(filepath = f\"{DATA_DIR}/encoding.json\")\n",
    "\n",
    "# some more variables\n",
    "include_velocity = (\"velocity\" in encoding[\"dimensions\"])\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))\n",
    "\n",
    "# helper function for displaying sequences\n",
    "def show(sequence: torch.tensor, columns: list = encoding[\"dimensions\"], show_index: bool = True):\n",
    "    start = perf_counter()\n",
    "    if not SHOW_SEQUENCES:\n",
    "        return\n",
    "    if len(sequence.shape) == 3:\n",
    "        sequence = sequence.squeeze(0)\n",
    "    sequence = pd.DataFrame(data = sequence, columns = columns)\n",
    "    # sequence.style.hide(axis = \"index\")\n",
    "    display(HTML(sequence.to_html(index = show_index)))\n",
    "    end = perf_counter()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prefix Sequence\n",
    "\n",
    "Prepare the prefix sequence by extracting relevant data from the MuseScore file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save prepared prefix sequence filepaths to\n",
    "paths = f\"{TEST_DATA_DIR}/paths.txt\"\n",
    "remove(paths)\n",
    "\n",
    "# helper function\n",
    "def prepare_prefix(music: BetterMusic, prefix_path: str):\n",
    "\n",
    "    # extract data from BetterMusic object\n",
    "    music.tracks = [music.tracks[0],] # make sure it is just one track\n",
    "    data = encode.extract_data(music = music, use_implied_duration = True, include_velocity = include_velocity, use_absolute_time = use_absolute_time)\n",
    "    show(sequence = data, columns = representation.DIMENSIONS)\n",
    "\n",
    "    # save encoded data\n",
    "    np.save(file = prefix_path, arr = data)\n",
    "\n",
    "    # text file with just the prefix path inside\n",
    "    with open(paths, \"a\") as paths_output:\n",
    "        paths_output.write(prefix_path + \"\\n\")\n",
    "\n",
    "# get BetterMusic object, both normal and expressive-feature-realized\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "for track in music.tracks:\n",
    "    for note in track.notes:\n",
    "        note.velocity = track.notes[0].velocity\n",
    "prepare_prefix(music = music, prefix_path = f\"{TEST_DATA_DIR}/{PREFIX_OUTPUT}.npy\")\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "music.realize_expressive_features()\n",
    "prepare_prefix(music = music, prefix_path = f\"{TEST_DATA_DIR}/{PREFIX_OUTPUT}.realized.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Models\n",
    "\n",
    "List the models that can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}/models.txt\", \"r\") as models_output: # read in list of trained models\n",
    "    models = [model.strip() for model in models_output.readlines()]\n",
    "    for model in models:\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Specify the model to evaluate (from the list generated above) by setting the `model` field below. Then, load in the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model to use\n",
    "model = \"anticipation_conditional_aug_ape_20M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get directories\n",
    "model_dir = f\"{DATA_DIR}/{model}\"\n",
    "evalqual_output_dir = f\"{model_dir}/evalqual\"\n",
    "if not exists(evalqual_output_dir):\n",
    "    makedirs(evalqual_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training configurations\n",
    "train_args = utils.load_json(filepath = f\"{model_dir}/train_args.json\")\n",
    "\n",
    "# set the device to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# create the dataset\n",
    "max_seq_len = train_args[\"max_seq_len\"]\n",
    "conditioning = train_args[\"conditioning\"]\n",
    "test_dataset = dataset.MusicDataset(paths = paths, encoding = encoding, conditioning = conditioning, max_seq_len = max_seq_len, use_augmentation = False, is_baseline = False)\n",
    "\n",
    "# create the model\n",
    "print(\"Creating model...\")\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))\n",
    "model = music_x_transformers.MusicXTransformer(\n",
    "    dim = train_args[\"dim\"],\n",
    "    encoding = encoding,\n",
    "    depth = train_args[\"layers\"],\n",
    "    heads = train_args[\"heads\"],\n",
    "    max_seq_len = max_seq_len,\n",
    "    max_temporal = encoding[\"max_\" + (\"time\" if use_absolute_time else \"beat\")],\n",
    "    rotary_pos_emb = train_args[\"rel_pos_emb\"],\n",
    "    use_abs_pos_emb = train_args[\"abs_pos_emb\"],\n",
    "    emb_dropout = train_args[\"dropout\"],\n",
    "    attn_dropout = train_args[\"dropout\"],\n",
    "    ff_dropout = train_args[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# load the checkpoint\n",
    "checkpoint_filepath = f\"{model_dir}/checkpoints/best_model.{train.PARTITIONS[1]}.pth\"\n",
    "model.load_state_dict(state_dict = torch.load(f = checkpoint_filepath, map_location = device))\n",
    "print(f\"Loaded model weights from: {checkpoint_filepath}\")\n",
    "model.eval()\n",
    "        \n",
    "# get special tokens\n",
    "sos = encoding[\"type_code_map\"][\"start-of-song\"]\n",
    "eos = encoding[\"type_code_map\"][\"end-of-song\"]\n",
    "note_token, grace_note_token = encoding[\"type_code_map\"][\"note\"], encoding[\"type_code_map\"][\"grace-note\"]\n",
    "expressive_feature_token = encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]\n",
    "is_anticipation = (conditioning == encode.CONDITIONINGS[-1])\n",
    "sigma = train_args[\"sigma\"] if use_absolute_time else encode.SIGMA_METRICAL\n",
    "\n",
    "# create data loader, get the singular batch\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset = test_dataset, num_workers = 4, collate_fn = dataset.MusicDataset.collate, batch_size = 1, shuffle = False)\n",
    "test_iter = iter(test_data_loader)\n",
    "def get_next_batch() -> torch.tensor:\n",
    "    batch = next(test_iter)\n",
    "    batch = batch[\"seq\"]\n",
    "    batch = batch[batch[:, :, 0] != eos].unsqueeze(dim = 0) # remove eos token from batch\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequences\n",
    "\n",
    "Now armed with the loaded model, generate a sequence given a prefix with and without expressive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Only\n",
    "\n",
    "Generate with a prefix of only notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure prefix is correct\n",
    "batch = get_next_batch()\n",
    "prefix_note = batch[batch[:, :, 0] != expressive_feature_token].unsqueeze(dim = 0) # filter out expressive features\n",
    "\n",
    "# generate new samples\n",
    "generated_note = model.generate(\n",
    "    seq_in = prefix_note,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"time\" if use_absolute_time else \"beat\"),\n",
    "    notes_only = True,\n",
    "    is_anticipation = is_anticipation,\n",
    "    sigma = sigma\n",
    ")\n",
    "show(generated_note)\n",
    "generated_note = torch.cat(tensors = (prefix_note, generated_note), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.note.npy\", arr = generated_note) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_note[0], encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_note = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.note.wav\"\n",
    "music.write(path = audio_output_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Expressive Features\n",
    "\n",
    "Generate with a prefix of notes and expressive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure prefix is correct\n",
    "batch = get_next_batch()\n",
    "prefix_total = batch\n",
    "\n",
    "# generate new samples\n",
    "generated_total = model.generate(\n",
    "    seq_in = prefix_total,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"time\" if use_absolute_time else \"beat\"),\n",
    "    notes_only = False,\n",
    "    is_anticipation = is_anticipation,\n",
    "    sigma = sigma\n",
    ")\n",
    "show(generated_total)\n",
    "generated_total = torch.cat(tensors = (prefix_total, generated_total), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.total.npy\", arr = generated_total) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_total[0], encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_total = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.total.wav\"\n",
    "music.write(path = audio_output_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Compare the Audios!\n",
    "\n",
    "Compare the `.wav` files -- did adding expressive features make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NOTES ONLY: {audio_output_note}\")\n",
    "display(Audio(audio_output_note))\n",
    "print(f\"EXPRESSIVE FEATURES: {audio_output_total}\")\n",
    "display(Audio(audio_output_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
