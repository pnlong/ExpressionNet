{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation of Model Performance\n",
    "\n",
    "Given the same prefix sequence of notes, does adding expressive features change model output? That is, do models respect expressive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Some constants like filepaths and encodings for running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data directory\n",
    "DATA_DIR = \"/home/pnlong/musescore/datavaa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from os.path import exists, basename\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import representation\n",
    "import utils\n",
    "import torch\n",
    "import dataset\n",
    "import music_x_transformers\n",
    "import train\n",
    "from encode import extract_data\n",
    "import decode\n",
    "from read_mscz.read_mscz import read_musescore\n",
    "\n",
    "# filepaths\n",
    "TEST_DATA_DIR = \"/home/pnlong/musescore/test_data/evalqual\"\n",
    "if not exists(TEST_DATA_DIR):\n",
    "    makedirs(TEST_DATA_DIR)\n",
    "PREFIX_MSCZ_FILEPATH = f\"{TEST_DATA_DIR}/test.mscz\"\n",
    "if not exists(PREFIX_MSCZ_FILEPATH):\n",
    "    raise FileNotFoundError(\"Must provide a valid MuseScore prefix filepath.\")\n",
    "PREFIX_OUTPUT = basename(PREFIX_MSCZ_FILEPATH)\n",
    "\n",
    "# load the encoding\n",
    "encoding = representation.load_encoding(filepath = f\"{DATA_DIR}/encoding.json\")\n",
    "\n",
    "# some more variables\n",
    "include_velocity = (\"velocity\" in encoding[\"dimensions\"])\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prefix Sequence\n",
    "\n",
    "Prepare the prefix sequence by extracting relevant data from the MuseScore file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BetterMusic object\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "music.realize_expressive_features()\n",
    "\n",
    "# extract data from BetterMusic object\n",
    "music.tracks = [music.tracks[0],] # make sure it is just one track\n",
    "data = extract_data(music = music, use_implied_duration = True, include_velocity = include_velocity, use_absolute_time = use_absolute_time)\n",
    "\n",
    "# save encoded data\n",
    "prefix_path = f\"{TEST_DATA_DIR}/{basename(PREFIX_MSCZ_FILEPATH)}.npy\"\n",
    "np.save(file = prefix_path, arr = data)\n",
    "\n",
    "# text file with just the prefix path inside\n",
    "paths = f\"{TEST_DATA_DIR}/paths.txt\"\n",
    "with open(paths, \"w\") as paths_output:\n",
    "    paths_output.write(prefix_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Models\n",
    "\n",
    "List the models that can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - baseline_aug_ape_31M\n",
      "  - prefix_aug_ape_31M\n",
      "  - anticipation_aug_ape_31M\n",
      "  - prefix_conditional_aug_ape_31M\n",
      "  - anticipation_conditional_aug_ape_31M\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_DIR}/models.txt\", \"r\") as models_output: # read in list of trained models\n",
    "    models = [model.strip() for model in models_output.readlines()]\n",
    "    for model in models:\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Specify the model to evaluate (from the list generated above) by setting the `model` field below. Then, load in the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model to use\n",
    "model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get directories\n",
    "model_dir = f\"{DATA_DIR}/{model}\"\n",
    "evalqual_output_dir = f\"{model_dir}/evalqual\"\n",
    "if not exists(evalqual_output_dir):\n",
    "    makedirs(evalqual_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Loaded model weights from: /home/pnlong/musescore/datavaa/baseline_aug_ape_31M/checkpoints/best_model.valid.pth\n"
     ]
    }
   ],
   "source": [
    "# load training configurations\n",
    "train_args = utils.load_json(filepath = f\"{model_dir}/train_args.json\")\n",
    "\n",
    "# set the device to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# create the dataset\n",
    "max_seq_len = train_args[\"max_seq_len\"]\n",
    "test_dataset = dataset.MusicDataset(paths = paths, encoding = encoding, max_seq_len = max_seq_len, use_augmentation = False, is_baseline = False)\n",
    "\n",
    "# create the model\n",
    "print(\"Creating model...\")\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))\n",
    "model = music_x_transformers.MusicXTransformer(\n",
    "    dim = train_args[\"dim\"],\n",
    "    encoding = encoding,\n",
    "    depth = train_args[\"layers\"],\n",
    "    heads = train_args[\"heads\"],\n",
    "    max_seq_len = max_seq_len,\n",
    "    max_temporal = encoding[\"max_\" + (\"time\" if use_absolute_time else \"beat\")],\n",
    "    rotary_pos_emb = train_args[\"rel_pos_emb\"],\n",
    "    use_abs_pos_emb = train_args[\"abs_pos_emb\"],\n",
    "    emb_dropout = train_args[\"dropout\"],\n",
    "    attn_dropout = train_args[\"dropout\"],\n",
    "    ff_dropout = train_args[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# load the checkpoint\n",
    "checkpoint_filepath = f\"{model_dir}/checkpoints/best_model.{train.PARTITIONS[1]}.pth\"\n",
    "model.load_state_dict(state_dict = torch.load(f = checkpoint_filepath, map_location = device))\n",
    "print(f\"Loaded model weights from: {checkpoint_filepath}\")\n",
    "model.eval()\n",
    "        \n",
    "# get special tokens\n",
    "sos = encoding[\"type_code_map\"][\"start-of-song\"]\n",
    "eos = encoding[\"type_code_map\"][\"end-of-song\"]\n",
    "note_token, grace_note_token = encoding[\"type_code_map\"][\"note\"], encoding[\"type_code_map\"][\"grace-note\"]\n",
    "expressive_token = encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]\n",
    "\n",
    "# create data loader, get the singular batch\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset = test_dataset, num_workers = 4, collate_fn = dataset.MusicDataset.collate, batch_size = 1, shuffle = False)\n",
    "test_iter = iter(test_data_loader)\n",
    "batch = next(test_iter)\n",
    "batch = batch[\"seq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequences\n",
    "\n",
    "Now armed with the loaded model, generate a sequence given a prefix with and without expressive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Only\n",
    "\n",
    "Generate with a prefix of only notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39msave(file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevalqual_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_OUTPUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.note.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr \u001b[38;5;241m=\u001b[39m generated_note) \u001b[38;5;66;03m# save as a numpy array\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# convert to audio\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m music \u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerated_note\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# convert to a BetterMusic object\u001b[39;00m\n\u001b[1;32m     20\u001b[0m audio_output_note \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevalqual_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_OUTPUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.note.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m music\u001b[38;5;241m.\u001b[39mwrite(path \u001b[38;5;241m=\u001b[39m audio_output_note)\n",
      "File \u001b[0;32m~/model_musescore/decode.py:280\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(codes, encoding)\u001b[0m\n\u001b[1;32m    277\u001b[0m data \u001b[38;5;241m=\u001b[39m decode_data(codes \u001b[38;5;241m=\u001b[39m codes, encoding \u001b[38;5;241m=\u001b[39m encoding)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# reconstruct the music object\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m music \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m music\n",
      "File \u001b[0;32m~/model_musescore/decode.py:136\u001b[0m, in \u001b[0;36mreconstruct\u001b[0;34m(data, resolution, encoding)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# iterate through data\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# get velocity\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     velocity \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m include_velocity \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_VELOCITY \u001b[38;5;66;03m# if there is a velocity value\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# get different fields, and make sure valid\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_absolute_time:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'pop'"
     ]
    }
   ],
   "source": [
    "# make sure prefix is correct\n",
    "prefix_note = batch[batch[:, :, 0] != encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]].unsqueeze(dim = 0) # filter out expressive features\n",
    "\n",
    "# generate new samples\n",
    "generated_note = model.generate(\n",
    "    seq_in = prefix_note,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"time\" if use_absolute_time else \"beat\"),\n",
    "    notes_only = True\n",
    ")\n",
    "generated_note = torch.cat(tensors = (prefix_note, generated_note), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.note.npy\", arr = generated_note) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_note[0], encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_note = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.note.wav\"\n",
    "music.write(path = audio_output_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Expressive Features\n",
    "\n",
    "Generate with a prefix of notes and expressive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure prefix is correct\n",
    "prefix_total = batch\n",
    "\n",
    "# generate new samples\n",
    "generated_total = model.generate(\n",
    "    seq_in = prefix_total,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"time\" if use_absolute_time else \"beat\"),\n",
    "    notes_only = True\n",
    ")\n",
    "generated_total = torch.cat(tensors = (prefix_total, generated_total), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.total.npy\", arr = generated_total) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_total[0], encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_total = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.total.wav\"\n",
    "music.write(path = audio_output_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Compare the Audios!\n",
    "\n",
    "Compare the `.wav` files -- did adding expressive features make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(\"NOTES ONLY\")\n",
    "IPython.display.display(IPython.display.Audio(audio_output_note))\n",
    "print(\"EXPRESSIVE FEATURES\")\n",
    "IPython.display.display(IPython.display.Audio(audio_output_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
