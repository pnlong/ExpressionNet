{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation of Model Performance\n",
    "\n",
    "Given the same prefix sequence of notes, does adding expressive features change model output? That is, do models respect expressive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Some constants like filepaths and encodings for running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from os.path import exists, basename\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import representation\n",
    "import utils\n",
    "import torch\n",
    "import dataset\n",
    "import music_x_transformers\n",
    "import train\n",
    "from encode import extract_data\n",
    "import decode\n",
    "from read_mscz.read_mscz import read_musescore\n",
    "\n",
    "# filepaths\n",
    "TEST_DATA_DIR = \"/home/pnlong/musescore/test_data/evalqual\"\n",
    "if not exists(TEST_DATA_DIR):\n",
    "    makedirs(TEST_DATA_DIR)\n",
    "PREFIX_MSCZ_FILEPATH = f\"{TEST_DATA_DIR}/test.mscz\"\n",
    "PREFIX_OUTPUT = basename(PREFIX_MSCZ_FILEPATH)\n",
    "DATA_DIR = f\"/home/pnlong/musescore/datava\"\n",
    "\n",
    "# load the encoding\n",
    "encoding = representation.load_encoding(filepath = f\"{DATA_DIR}/encoding.json\")\n",
    "\n",
    "# some more variables\n",
    "include_velocity = (\"velocity\" in encoding[\"dimensions\"])\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prefix Sequence\n",
    "\n",
    "Prepare the prefix sequence by extracting relevant data from the MuseScore file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BetterMusic object\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "music.realize_expressive_features()\n",
    "\n",
    "# extract data from BetterMusic object\n",
    "music.tracks = [music.tracks[0],] # make sure it is just one track\n",
    "data = extract_data(music = music, use_implied_duration = True, include_velocity = include_velocity, use_absolute_time = use_absolute_time)\n",
    "\n",
    "# save encoded data\n",
    "prefix_path = f\"{TEST_DATA_DIR}/{basename(PREFIX_MSCZ_FILEPATH)}.npy\"\n",
    "np.save(file = prefix_path, arr = data)\n",
    "\n",
    "# text file with just the prefix path inside\n",
    "paths = f\"{TEST_DATA_DIR}/paths.txt\"\n",
    "with open(paths, \"w\") as paths_output:\n",
    "    paths_output.write(prefix_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Models\n",
    "\n",
    "List the models that can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}/models.txt\", \"r\") as models_output: # read in list of trained models\n",
    "    models = [model.strip() for model in models_output.readlines()]\n",
    "    for model in models:\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Specify the model to evaluate (from the list generated above) by setting the `model` field below. Then, load in the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"\" # SPECIFY\n",
    "model_dir = f\"{DATA_DIR}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training configurations\n",
    "train_args = utils.load_json(filepath = f\"{model_dir}/train_args.json\")\n",
    "\n",
    "# set the device to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# create the dataset\n",
    "max_seq_len = train_args[\"max_seq_len\"]\n",
    "test_dataset = dataset.MusicDataset(paths = paths, encoding = encoding, max_seq_len = max_seq_len, use_augmentation = False, is_baseline = False)\n",
    "\n",
    "# create the model\n",
    "print(\"Creating model...\")\n",
    "use_absolute_time = not ((\"beat\" in encoding[\"dimensions\"]) and (\"position\" in encoding[\"dimensions\"]))\n",
    "model = music_x_transformers.MusicXTransformer(\n",
    "    dim = train_args[\"dim\"],\n",
    "    encoding = encoding,\n",
    "    depth = train_args[\"layers\"],\n",
    "    heads = train_args[\"heads\"],\n",
    "    max_seq_len = max_seq_len,\n",
    "    max_temporal = encoding[\"max_\" + (\"time\" if use_absolute_time else \"beat\")],\n",
    "    rotary_pos_emb = train_args[\"rel_pos_emb\"],\n",
    "    use_abs_pos_emb = train_args[\"abs_pos_emb\"],\n",
    "    emb_dropout = train_args[\"dropout\"],\n",
    "    attn_dropout = train_args[\"dropout\"],\n",
    "    ff_dropout = train_args[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# load the checkpoint\n",
    "checkpoint_filepath = f\"{model_dir}/checkpoints/best_model.{train.PARTITIONS[1]}.pth\"\n",
    "model.load_state_dict(state_dict = torch.load(f = checkpoint_filepath, map_location = device))\n",
    "print(f\"Loaded model weights from: {checkpoint_filepath}\")\n",
    "model.eval()\n",
    "        \n",
    "# get special tokens\n",
    "sos = encoding[\"type_code_map\"][\"start-of-song\"]\n",
    "eos = encoding[\"type_code_map\"][\"end-of-song\"]\n",
    "note_token, grace_note_token = encoding[\"type_code_map\"][\"note\"], encoding[\"type_code_map\"][\"grace-note\"]\n",
    "expressive_token = encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]\n",
    "\n",
    "# create data loader, get the singular batch\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset = test_dataset, num_workers = 4, collate_fn = dataset.MusicDataset.collate, batch_size = 1, shuffle = False)\n",
    "test_iter = iter(test_data_loader)\n",
    "batch = next(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequences\n",
    "\n",
    "Now armed with the loaded model, generate a sequence given a prefix with and without expressive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Only\n",
    "\n",
    "Generate with a prefix of only notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure prefix is correct\n",
    "prefix_note = batch[batch[:, :, 0] != encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]] # filter out expressive features\n",
    "\n",
    "# generate new samples\n",
    "generated_note = model.generate(\n",
    "    seq_in = prefix_note,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"beat\"),\n",
    "    notes_only = True\n",
    ")\n",
    "generated_note = torch.cat(tensors = (prefix_note, generated_note), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{PREFIX_OUTPUT}.note.npy\", arr = generated_note) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_note, encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_note = f\"{PREFIX_OUTPUT}.note.wav\"\n",
    "music.write(path = audio_output_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Expressive Features\n",
    "\n",
    "Generate with a prefix of notes and expressive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure prefix is correct\n",
    "prefix_total = batch\n",
    "\n",
    "# generate new samples\n",
    "generated_total = model.generate(\n",
    "    seq_in = prefix_total,\n",
    "    seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "    eos_token = eos,\n",
    "    temperature = 1.0,\n",
    "    filter_logits_fn = \"top_k\",\n",
    "    filter_thres = 0.9,\n",
    "    monotonicity_dim = (\"type\", \"beat\"),\n",
    "    notes_only = True\n",
    ")\n",
    "generated_total = torch.cat(tensors = (prefix_total, generated_total), dim = 1).numpy() # wrangle a bit\n",
    "np.save(file = f\"{PREFIX_OUTPUT}.total.npy\", arr = generated_total) # save as a numpy array\n",
    "\n",
    "# convert to audio\n",
    "music = decode.decode(codes = generated_total, encoding = encoding) # convert to a BetterMusic object\n",
    "audio_output_total = f\"{PREFIX_OUTPUT}.total.wav\"\n",
    "music.write(path = audio_output_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Compare the Audios!\n",
    "\n",
    "Compare the `.wav` files -- did adding expressive features make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(\"NOTES ONLY\")\n",
    "IPython.display.display(IPython.display.Audio(audio_output_note))\n",
    "print(\"EXPRESSIVE FEATURES\")\n",
    "IPython.display.display(IPython.display.Audio(audio_output_total))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
